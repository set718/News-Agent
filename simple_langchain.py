"""
ç®€åŒ–çš„Langchainæ¶æ„å®ç°
é¿å…å¤æ‚çš„ä¾èµ–å†²çªï¼Œä½¿ç”¨æ ¸å¿ƒLangchainæ¦‚å¿µé‡æ„ç³»ç»Ÿ
"""
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

from openai import OpenAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field

from email_fetcher import EmailFetcher
from data_storage import db_manager
from config import DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, CONTENT_FILTER_PROMPT, LANGCHAIN_BATCH_SIZE


class FilterResult(BaseModel):
    """ç­›é€‰ç»“æœæ¨¡å‹"""
    is_selected: bool = Field(description="æ˜¯å¦é€šè¿‡ç­›é€‰")
    quality_score: float = Field(description="è´¨é‡è¯„åˆ†(1-10)", ge=1, le=10)
    relevance_score: float = Field(description="ç›¸å…³æ€§è¯„åˆ†(1-10)", ge=1, le=10)
    reason: str = Field(description="ç­›é€‰ç†ç”±")
    key_points: List[str] = Field(description="å…³é”®è¦ç‚¹", default=[])
    category: str = Field(description="æ–‡ç« åˆ†ç±»", default="å…¶ä»–")


class LangchainProcessor:
    """åŸºäºLangchainçš„ç®€åŒ–å¤„ç†å™¨"""
    
    def __init__(self, batch_size: int = None):
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )
        
        # è®¾ç½®æ‰¹é‡å¤„ç†å¤§å°
        self.batch_size = batch_size or LANGCHAIN_BATCH_SIZE
        
        # åˆå§‹åŒ–æç¤ºæ¨¡æ¿
        self.filter_prompt = PromptTemplate(
            template=CONTENT_FILTER_PROMPT,
            input_variables=["title", "source", "summary", "publish_time", "url"]
        )
        
        # åˆå§‹åŒ–æ‰¹é‡ç­›é€‰æç¤ºæ¨¡æ¿
        self.batch_filter_prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»å†…å®¹åˆ†æå¸ˆï¼Œè´Ÿè´£è¯„ä¼°å’Œç­›é€‰æ–°é—»æ–‡ç« çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚

ç›®æ ‡ç”¨æˆ·ï¼šæ±½è½¦è¡Œä¸šåˆ¶é€ å·¥ç¨‹å¸ˆ
å…³æ³¨é¢†åŸŸï¼šæ±½è½¦å·¥å‚å»ºè®¾ã€AIæŠ€æœ¯ã€å…ˆè¿›åˆ¶é€ æŠ€æœ¯

ç­›é€‰æ ‡å‡†ï¼š
1. ä¼˜å…ˆä¿ç•™ï¼š
   - æ±½è½¦å·¥å‚å»ºè®¾ã€æ‰©å»ºã€æŠ€æœ¯å‡çº§ç›¸å…³
   - æ±½è½¦åˆ¶é€ æµç¨‹ã€ç”Ÿäº§çº¿ã€è´¨é‡æ§åˆ¶
   - å¯åº”ç”¨äºæ±½è½¦å·¥å‚çš„AIæŠ€æœ¯ï¼ˆå·¥ä¸šæœºå™¨äººã€æœºå™¨è§†è§‰ã€æ•°å­—å­ªç”Ÿã€é¢„æµ‹æ€§ç»´æŠ¤ç­‰ï¼‰
   - å…ˆè¿›åˆ¶é€ æŠ€æœ¯ï¼ˆå¢æåˆ¶é€ /3Dæ‰“å°ã€è‡ªåŠ¨åŒ–ã€æ™ºèƒ½åˆ¶é€ ã€å·¥ä¸š4.0ç­‰ï¼‰
   - æ±½è½¦ä¾›åº”é“¾ã€ææ–™æŠ€æœ¯ã€æ–°èƒ½æºæ±½è½¦åˆ¶é€ 

2. ä¿ç•™ä½†é™ä½ä¼˜å…ˆçº§ï¼š
   - é€šç”¨åˆ¶é€ æŠ€æœ¯ï¼ˆå¦‚æœå¯åº”ç”¨äºæ±½è½¦å·¥å‚ï¼‰
   - å…¶ä»–è¡Œä¸šçš„å…ˆè¿›åˆ¶é€ æ¡ˆä¾‹ï¼ˆå¦‚æœæŠ€æœ¯å¯å€Ÿé‰´ï¼‰

3. æ˜ç¡®å‰”é™¤ï¼š
   - æ”¿æ²»ã€ç¤¾ä¼šã€å¨±ä¹æ–°é—»
   - ä»…æ¶‰åŠéæ±½è½¦è¡Œä¸šåˆ¶é€ çš„å†…å®¹
   - æ±½è½¦é”€å”®ã€å¸‚åœºè¥é”€ã€é‡‘èæŠ•èµ„ç±»æ–°é—»
   - ä¸åˆ¶é€ å·¥ç¨‹æ— å…³çš„æ±½è½¦æ–°é—»ï¼ˆå¦‚è½¦å‹å‘å¸ƒã€æµ‹è¯„ç­‰ï¼‰

è¯·åˆ†æä»¥ä¸‹ {num_articles} ç¯‡æ–°é—»ï¼Œå¹¶å¯¹æ¯ç¯‡æ–‡ç« è¿”å›ç­›é€‰ç»“æœï¼š

{articles_content}

è¯·è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å« {num_articles} ä¸ªç­›é€‰ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "is_selected": true/false,
        "quality_score": 1-10çš„è¯„åˆ†ï¼ˆå†…å®¹æ·±åº¦å’Œä»·å€¼ï¼‰,
        "relevance_score": 1-10çš„è¯„åˆ†ï¼ˆä¸æ±½è½¦åˆ¶é€ å·¥ç¨‹çš„ç›¸å…³æ€§ï¼‰,
        "reason": "è¯¦ç»†çš„ç­›é€‰ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©æˆ–æ‹’ç»",
        "key_points": ["æå–çš„å…³é”®æŠ€æœ¯è¦ç‚¹æˆ–åˆ¶é€ ä¿¡æ¯"],
        "category": "åˆ†ç±»ï¼šæ±½è½¦å·¥å‚å»ºè®¾/AIåˆ¶é€ æŠ€æœ¯/å…ˆè¿›åˆ¶é€ /ä¾›åº”é“¾æŠ€æœ¯/å…¶ä»–"
    }},
    ...
]""",
            input_variables=["num_articles", "articles_content"]
        )
        
        # åˆå§‹åŒ–é‚®ä»¶è·å–å™¨
        self.email_fetcher = EmailFetcher()
        
        print(f"ç®€åŒ–Langchainå¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ (æ‰¹é‡å¤§å°: {self.batch_size})")
    
    def fetch_emails_chain(self, days: int = 1) -> Dict[str, Any]:
        """é‚®ä»¶è·å–é“¾"""
        print(f"\næ‰§è¡Œé‚®ä»¶è·å–é“¾ (æœ€è¿‘ {days} å¤©)")
        
        try:
            # è·å–é‚®ä»¶
            alert_emails = self.email_fetcher.fetch_google_alerts(days=days)
            
            if not alert_emails:
                print("æ²¡æœ‰æ‰¾åˆ°æ–°çš„Google Alerté‚®ä»¶")
                return {"emails": [], "articles_count": 0}
            
            # å­˜å‚¨é‚®ä»¶å’Œæ–‡ç« 
            total_new_articles = 0
            stored_emails = []
            
            for email in alert_emails:
                # ä¿å­˜é‚®ä»¶
                saved_email = db_manager.save_alert_email({
                    'message_id': email.message_id,
                    'subject': email.subject,
                    'sender': email.sender,
                    'date': email.date,
                    'body_html': email.body_html,
                    'body_text': email.body_text
                })
                
                if saved_email and email.articles:
                    for article in email.articles:
                        article['email_message_id'] = email.message_id
                    
                    saved_articles = db_manager.save_articles(email.articles)
                    total_new_articles += len(saved_articles)
                    stored_emails.append(email)
            
            result = {
                "emails": stored_emails,
                "articles_count": total_new_articles
            }
            
            print(f"é‚®ä»¶è·å–å®Œæˆ: {len(stored_emails)} å°é‚®ä»¶, {total_new_articles} ç¯‡æ–°æ–‡ç« ")
            return result
            
        except Exception as e:
            print(f"é‚®ä»¶è·å–å¤±è´¥: {e}")
            return {"emails": [], "articles_count": 0}
    
    def filter_articles_chain(self, limit: int = None) -> Dict[str, Any]:
        """æ–‡ç« ç­›é€‰é“¾ï¼ˆæ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if limit:
            print(f"\næ‰§è¡Œæ–‡ç« ç­›é€‰é“¾ (é™åˆ¶ {limit} ç¯‡ï¼Œæ‰¹é‡å¤§å°: {self.batch_size})")
        else:
            print(f"\næ‰§è¡Œæ–‡ç« ç­›é€‰é“¾ (å¤„ç†æ‰€æœ‰æœªç­›é€‰æ–‡ç« ï¼Œæ‰¹é‡å¤§å°: {self.batch_size})")
        
        # è·å–æœªç­›é€‰çš„æ–‡ç« 
        unfiltered_articles = db_manager.get_unfiltered_articles(limit=limit)
        
        if not unfiltered_articles:
            print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç­›é€‰çš„æ–‡ç« ")
            return {'total': 0, 'processed': 0, 'selected': 0, 'rejected': 0, 'failed': 0}
        
        print(f"å¼€å§‹æ‰¹é‡ç­›é€‰ {len(unfiltered_articles)} ç¯‡æ–‡ç« ...")
        
        stats = {
            'total': len(unfiltered_articles),
            'processed': 0,
            'selected': 0,
            'rejected': 0,
            'failed': 0
        }
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†æ–‡ç« 
        for batch_start in range(0, len(unfiltered_articles), self.batch_size):
            batch_end = min(batch_start + self.batch_size, len(unfiltered_articles))
            batch_articles = unfiltered_articles[batch_start:batch_end]
            
            batch_num = batch_start // self.batch_size + 1
            total_batches = (len(unfiltered_articles) + self.batch_size - 1) // self.batch_size
            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}: æ–‡ç«  {batch_start+1}-{batch_end}")
            
            try:
                # æ‰¹é‡ç­›é€‰è¿™ç»„æ–‡ç« 
                batch_results = self._filter_batch_articles(batch_articles)
                
                # å¤„ç†ç»“æœ
                for i, (article, result) in enumerate(zip(batch_articles, batch_results)):
                    if result:
                        # æ›´æ–°æ•°æ®åº“
                        success = db_manager.update_article_filter_result(article.id, result)
                        if success:
                            stats['processed'] += 1
                            if result.get('is_selected'):
                                stats['selected'] += 1
                                print(f"  æ–‡ç«  {batch_start+i+1}: é€šè¿‡")
                            else:
                                stats['rejected'] += 1
                                print(f"  æ–‡ç«  {batch_start+i+1}: æœªé€šè¿‡")
                        else:
                            stats['failed'] += 1
                            print(f"  æ–‡ç«  {batch_start+i+1}: æ•°æ®åº“æ›´æ–°å¤±è´¥")
                    else:
                        stats['failed'] += 1
                        print(f"  æ–‡ç«  {batch_start+i+1}: ç­›é€‰å¤±è´¥")
                
            except Exception as e:
                print(f"  æ‰¹æ¬¡ç­›é€‰å¤±è´¥: {e}")
                # å¦‚æœæ‰¹æ¬¡å¤±è´¥ï¼Œæ ‡è®°è¿™æ‰¹æ¬¡æ‰€æœ‰æ–‡ç« ä¸ºå¤±è´¥
                stats['failed'] += len(batch_articles)
        
        print(f"\næ‰¹é‡ç­›é€‰å®Œæˆ:")
        print(f"  - æ€»è®¡: {stats['total']}")
        print(f"  - æˆåŠŸå¤„ç†: {stats['processed']}")
        print(f"  - ç­›é€‰é€šè¿‡: {stats['selected']}")
        print(f"  - ç­›é€‰æœªé€šè¿‡: {stats['rejected']}")
        print(f"  - å¤„ç†å¤±è´¥: {stats['failed']}")
        
        return stats
    
    def _filter_batch_articles(self, articles: List) -> List[Optional[Dict]]:
        """æ‰¹é‡ç­›é€‰æ–‡ç« """
        try:
            # æ„å»ºæ–‡ç« å†…å®¹
            articles_content = ""
            for i, article in enumerate(articles, 1):
                articles_content += f"""
æ–‡ç«  {i}:
æ ‡é¢˜ï¼š{article.title}
æ¥æºï¼š{article.source or "æœªçŸ¥æ¥æº"}
å†…å®¹æ‘˜è¦ï¼š{article.summary or "æ— æ‘˜è¦"}
å‘å¸ƒæ—¶é—´ï¼š{article.publish_time or "æœªçŸ¥æ—¶é—´"}
åŸæ–‡é“¾æ¥ï¼š{article.url[:100] + "..." if len(article.url) > 100 else article.url}

"""
            
            # ä½¿ç”¨æ‰¹é‡æç¤ºæ¨¡æ¿
            prompt = self.batch_filter_prompt.format(
                num_articles=len(articles),
                articles_content=articles_content.strip()
            )
            
            # è°ƒç”¨LLM
            response = self.llm.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»å†…å®¹åˆ†æå¸ˆï¼Œè´Ÿè´£è¯„ä¼°å’Œç­›é€‰æ–°é—»æ–‡ç« çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ•°ç»„æ ¼å¼è¿”å›åˆ†æç»“æœã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=4000  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ‰¹é‡å¤„ç†
            )
            
            # è§£æå“åº”
            return self._parse_batch_response(response.choices[0].message.content, len(articles))
            
        except Exception as e:
            print(f"æ‰¹é‡ç­›é€‰å¤±è´¥: {e}")
            return [None] * len(articles)

    def _parse_batch_response(self, response: str, expected_count: int) -> List[Optional[Dict]]:
        """è§£ææ‰¹é‡APIå“åº”"""
        try:
            # å°è¯•æå–JSONæ•°ç»„
            response = response.strip()
            
            # æŸ¥æ‰¾JSONæ•°ç»„å¼€å§‹å’Œç»“æŸä½ç½®
            start_idx = response.find('[')
            end_idx = response.rfind(']') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                results = json.loads(json_str)
                
                if not isinstance(results, list):
                    print(f"å“åº”ä¸æ˜¯JSONæ•°ç»„æ ¼å¼")
                    return [None] * expected_count
                
                # éªŒè¯ç»“æœæ•°é‡
                if len(results) != expected_count:
                    print(f"å“åº”æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {expected_count}ï¼Œå®é™… {len(results)}")
                    # è°ƒæ•´ç»“æœæ•°é‡
                    if len(results) < expected_count:
                        results.extend([None] * (expected_count - len(results)))
                    else:
                        results = results[:expected_count]
                
                # éªŒè¯å’Œæ¸…ç†æ¯ä¸ªç»“æœ
                cleaned_results = []
                for i, result in enumerate(results):
                    if result and isinstance(result, dict):
                        # éªŒè¯å¿…éœ€å­—æ®µ
                        required_fields = ['is_selected', 'quality_score', 'relevance_score', 'reason']
                        if all(field in result for field in required_fields):
                            # ç¡®ä¿åˆ†æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
                            result['quality_score'] = max(1, min(10, float(result.get('quality_score', 5))))
                            result['relevance_score'] = max(1, min(10, float(result.get('relevance_score', 5))))
                            # ç¡®ä¿key_pointsæ˜¯åˆ—è¡¨
                            if 'key_points' not in result:
                                result['key_points'] = []
                            elif not isinstance(result['key_points'], list):
                                result['key_points'] = [str(result['key_points'])]
                            # ç¡®ä¿categoryå­˜åœ¨
                            if 'category' not in result:
                                result['category'] = 'å…¶ä»–'
                            
                            cleaned_results.append(result)
                        else:
                            print(f"æ–‡ç«  {i+1} å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {result}")
                            cleaned_results.append(None)
                    else:
                        print(f"æ–‡ç«  {i+1} å“åº”æ ¼å¼é”™è¯¯")
                        cleaned_results.append(None)
                
                return cleaned_results
            else:
                print(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆJSONæ•°ç»„: {response[:200]}...")
                return [None] * expected_count
                
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {response[:500]}...")
            return [None] * expected_count
        except Exception as e:
            print(f"è§£ææ‰¹é‡å“åº”æ—¶å‡ºé”™: {e}")
            return [None] * expected_count
    
    def _filter_single_article(self, article) -> Optional[Dict]:
        """ä½¿ç”¨Langchainç­›é€‰å•ç¯‡æ–‡ç« """
        try:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = {
                "title": article.title,
                "source": article.source or "æœªçŸ¥æ¥æº",
                "summary": article.summary or "æ— æ‘˜è¦",
                "publish_time": article.publish_time or "æœªçŸ¥æ—¶é—´",
                "url": article.url[:100] + "..." if len(article.url) > 100 else article.url
            }
            
            # ä½¿ç”¨æç¤ºæ¨¡æ¿ç”Ÿæˆå®Œæ•´æç¤º
            prompt = self.filter_prompt.format(**input_data)
            
            # è°ƒç”¨LLM
            response = self.llm.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»å†…å®¹åˆ†æå¸ˆï¼Œè´Ÿè´£è¯„ä¼°å’Œç­›é€‰æ–°é—»æ–‡ç« çš„è´¨é‡å’Œç›¸å…³æ€§ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            # è§£æå“åº”
            return self._parse_llm_response(response.choices[0].message.content)
            
        except Exception as e:
            print(f"ç­›é€‰æ–‡ç« æ—¶å‡ºé”™: {e}")
            return None
    
    def _parse_llm_response(self, response: str) -> Optional[Dict]:
        """è§£æLLMå“åº”"""
        try:
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            response = response.strip()
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                result = json.loads(json_str)
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                required_fields = ['is_selected', 'quality_score', 'relevance_score', 'reason']
                if all(field in result for field in required_fields):
                    # ç¡®ä¿åˆ†æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    result['quality_score'] = max(1, min(10, float(result.get('quality_score', 5))))
                    result['relevance_score'] = max(1, min(10, float(result.get('relevance_score', 5))))
                    
                    # ç¡®ä¿åˆ—è¡¨å­—æ®µå­˜åœ¨
                    if 'key_points' not in result:
                        result['key_points'] = []
                    if 'category' not in result:
                        result['category'] = "å…¶ä»–"
                    
                    return result
                else:
                    print(f"å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {result}")
                    return None
            else:
                print(f"æ— æ³•æ‰¾åˆ°æœ‰æ•ˆJSON: {response}")
                return None
                
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            return None
    
    def generate_report_chain(self, days: int = 1) -> str:
        """æŠ¥å‘Šç”Ÿæˆé“¾"""
        print(f"\næ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆé“¾ (æœ€è¿‘ {days} å¤©)")
        
        try:
            # è·å–æ•°æ®åº“ç»Ÿè®¡
            stats = db_manager.get_statistics()
            
            print(f"æ•°æ®åº“ç»Ÿè®¡:")
            print(f"  - æ€»é‚®ä»¶æ•°: {stats['total_emails']}")
            print(f"  - æ€»æ–‡ç« æ•°: {stats['total_articles']}")
            print(f"  - å·²ç­›é€‰: {stats['filtered_articles']}")
            print(f"  - ç­›é€‰é€šè¿‡: {stats['selected_articles']}")
            
            # è·å–ç­›é€‰é€šè¿‡çš„æ–‡ç« 
            selected_articles = db_manager.get_selected_articles(days=days)
            
            if not selected_articles:
                report = f"æœ€è¿‘ {days} å¤©æ²¡æœ‰ç­›é€‰é€šè¿‡çš„æ–‡ç« ã€‚"
                return report
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report = self._generate_detailed_report(selected_articles, days)
            
            print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return report
            
        except Exception as e:
            print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return ""
    
    def _generate_detailed_report(self, articles, days: int) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        # ç»Ÿè®¡åˆ†æ
        source_stats = {}
        category_stats = {}
        quality_scores = []
        relevance_scores = []
        
        for article in articles:
            # æ¥æºç»Ÿè®¡
            source = article.source or "æœªçŸ¥æ¥æº"
            source_stats[source] = source_stats.get(source, 0) + 1
            
            # ç±»åˆ«ç»Ÿè®¡
            if article.category:
                category_stats[article.category] = category_stats.get(article.category, 0) + 1
            
            # åˆ†æ•°æ”¶é›†
            if article.quality_score:
                quality_scores.append(article.quality_score)
            if article.relevance_score:
                relevance_scores.append(article.relevance_score)
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
æœ€è¿‘ {days} å¤©ç­›é€‰æŠ¥å‘Š (Langchainæ¶æ„)
{'='*50}

æ€»ä½“ç»Ÿè®¡:
- ç­›é€‰é€šè¿‡æ–‡ç« : {len(articles)} ç¯‡
- å¹³å‡è´¨é‡è¯„åˆ†: {avg_quality:.1f}/10
- å¹³å‡ç›¸å…³æ€§è¯„åˆ†: {avg_relevance:.1f}/10

æ¥æºåˆ†å¸ƒ:
"""
        
        # æ·»åŠ æ¥æºç»Ÿè®¡
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            report += f"- {source}: {count} ç¯‡\n"
        
        # æ·»åŠ ç±»åˆ«ç»Ÿè®¡
        if category_stats:
            report += "\nç±»åˆ«åˆ†å¸ƒ:\n"
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                report += f"- {category}: {count} ç¯‡\n"
        
        # æ·»åŠ é«˜è´¨é‡æ–‡ç« æ¨è
        top_articles = sorted(articles, 
                            key=lambda x: (x.quality_score or 0) + (x.relevance_score or 0), 
                            reverse=True)[:5]
        
        if top_articles:
            report += "\næ¨èé˜…è¯» (Top 5):\n"
            for i, article in enumerate(top_articles, 1):
                report += f"{i}. {article.title}\n"
                report += f"   æ¥æº: {article.source} | è´¨é‡: {article.quality_score:.1f} | ç›¸å…³æ€§: {article.relevance_score:.1f}\n"
                report += f"   é“¾æ¥: {article.url}\n\n"
        
        return report
    
    def run_full_workflow(self, days: int = 1, filter_limit: int = None, auto_export_excel: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„Langchainå·¥ä½œæµç¨‹"""
        print(f"å¯åŠ¨Langchainæ–°é—»å¤„ç†å·¥ä½œæµ")
        print(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # æ­¥éª¤1: é‚®ä»¶è·å–é“¾
            email_result = self.fetch_emails_chain(days=days)
            
            # æ­¥éª¤2: æ–‡ç« ç­›é€‰é“¾
            filter_result = self.filter_articles_chain(limit=filter_limit)
            
            # æ­¥éª¤3: æŠ¥å‘Šç”Ÿæˆé“¾
            report = self.generate_report_chain(days=days)
            
            # æ­¥éª¤4: è‡ªåŠ¨å¯¼å‡ºExcel (å¦‚æœå¯ç”¨)
            excel_file = ""
            if auto_export_excel:
                excel_file = self._auto_export_excel()
            
            # ç»„åˆç»“æœ
            result = {
                'emails_processed': len(email_result["emails"]),
                'articles_added': email_result["articles_count"],
                'filter_stats': filter_result,
                'report': report,
                'excel_file': excel_file,
                'workflow_status': 'completed',
                'architecture': 'langchain'
            }
            
            # è¾“å‡ºæœ€ç»ˆç»“æœ
            print(f"\n{'='*50}")
            print(f"ğŸ‰ Langchainå·¥ä½œæµç¨‹å®Œæˆ")
            print(f"{'='*50}")
            
            if report:
                print(report)
            
            if excel_file:
                print(f"\nå·²è‡ªåŠ¨å¯¼å‡ºå½“æ—¥Excelæ–‡ä»¶: {excel_file}")
            
            return result
            
        except Exception as e:
            print(f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return {
                'emails_processed': 0,
                'articles_added': 0,
                'filter_stats': {},
                'report': "",
                'excel_file': "",
                'workflow_status': 'failed',
                'error': str(e),
                'architecture': 'langchain'
            }
    
    def _auto_export_excel(self) -> str:
        """è‡ªåŠ¨å¯¼å‡ºå½“å¤©çš„Excelæ–‡ä»¶"""
        try:
            print(f"\nè‡ªåŠ¨å¯¼å‡ºå½“æ—¥Excelæ–‡ä»¶...")
            
            # å¯¼å…¥excel_exporter
            from excel_exporter import excel_exporter
            
            # å¯¼å‡ºä»Šå¤©çš„æ•°æ® (1å¤©)
            excel_file = excel_exporter.export_selected_articles(days=1)
            
            if excel_file:
                print(f"Excelæ–‡ä»¶å·²ç”Ÿæˆ: {excel_file}")
                return excel_file
            else:
                print("ä»Šå¤©æ²¡æœ‰ç­›é€‰é€šè¿‡çš„æ–‡ç« ï¼Œæœªç”ŸæˆExcelæ–‡ä»¶")
                return ""
                
        except Exception as e:
            print(f"Excelå¯¼å‡ºå¤±è´¥: {e}")
            return ""


if __name__ == "__main__":
    # æµ‹è¯•ç®€åŒ–Langchainå¤„ç†å™¨
    if not DEEPSEEK_API_KEY:
        print("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
        exit(1)
    
    try:
        print("ğŸ§ª æµ‹è¯•ç®€åŒ–Langchainæ¶æ„...")
        
        processor = LangchainProcessor()
        
        # æµ‹è¯•å®Œæ•´å·¥ä½œæµ
        result = processor.run_full_workflow(days=1, filter_limit=3)
        
        print(f"\næµ‹è¯•å®Œæˆï¼")
        print(f"æ¶æ„: {result.get('architecture', 'unknown')}")
        print(f"çŠ¶æ€: {result.get('workflow_status', 'unknown')}")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
